<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs" />
    <meta property="og:title" content="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs" />
    <meta
      property="og:description"
      content="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs"
    />
    <meta property="og:url" content="https://matthewyryang.github.io/e3" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/overview.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <meta name="twitter:title" content="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs" />
    <meta
      name="twitter:description"
      content="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs"
    />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta
      name="twitter:image"
      content="static/image/overview.png"
    />
    <meta name="twitter:card" content="static/image/overview.png" />
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>e3</title>
    <link rel="icon" type="image/x-icon" href="static/images/icon.png" />
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="static/css/bulma.min.css" />
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="static/css/index.css" />
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
      tailwind.config = {
        prefix: 'tw-', // This makes all Tailwind classes start with 'tw-'
      };
    </script>
  </head>
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h2 class="title is-2 publication-title ">
                <span style="font-family: monospace;">e3</span>: Learning to <u>E</u>xplore <u>E</u>nables <u>E</u>xtrapolation of Test-Time Compute for LLMs
              </h2>
              <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://ars22.github.io/" target="_blank"
                    >Amrith Setlur<sup>*1</sup></a
                  >
                  ,
                </span>
                <span class="author-block">
                  <a href="https://matthewyryang.com/" target="_blank"
                    >Matthew Y. R. Yang<sup>*1</sup></a
                  >
                  ,
                </span>
                <span class="author-block">
                  <a href="https://sea-snell.github.io/" target="_blank"
                    >Charlie Snell<sup>2</sup></a
                  >
                  ,
                </span>
                <span class="author-block">
                  <a href="https://github.com/jgreer013/" target="_blank"
                    >Jeremy Greer<sup>3</sup></a
                  >
                  ,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=QdfBJmAAAAAJ&hl=en" target="_blank"
                    >Ian Wu<sup>1</sup></a
                  >
                  ,
                </span>
                <span class="author-block">
                  <a href="https://www.cs.cmu.edu/~smithv/" target="_blank"
                    >Virginia Smith<sup>1</sup></a
                  >
                  ,
                </span>
                <span class="author-block">
                  <a href="https://msimchowitz.github.io/" target="_blank"
                    >Max Simchowitz<sup>1</sup></a
                  >
                  ,
                </span>
                <span class="author-block">
                  <a href="https://aviralkumar2907.github.io/" target="_blank"
                    >Aviral Kumar<sup>1</sup></a
                  >
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <sup>1</sup>Carnegie Mellon University,
                  <sup>2</sup>UC Berkeley,
                  <sup>3</sup>Oumi,
                  <sup>*</sup>Equal Contribution
                </span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a
                      href=""
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                  <!-- <span class="link-block">
                    <a
                      href="https://www.youtube.com/watch?v=Qv8aTLthfhs"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a
                      href=""
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="has-text-centered">
        <div class="text-center">
          <img src="static/images/fig1-gif.gif" alt="e3: Exploration Enables Extrapolation" class="mx-auto"/>
        </div>
      </div>
      <p class="tw-text-left tw-text-sm tw-text-gray-500 tw-mt-4">
        <b>In-context <u>e</u>xploration <u>e</u>nables <u>e</u>xtrapolation
        of test-time compute (<span style="font-family: monospace;">e3</span>)</b>:  (i) by chaining asymmetric capabilities of the base model, <i>e.g.,</i> reliably self-verifying responses after generating them; (ii) lengthening model responses by chaining more asymmetries until the correct answer is discovered by utilizing the "negative" part of the RL policy gradient generated from incorrect responses; and (iii) coupling data & budget curricula for RL training that carefully structures exploration by sequentially training models on different datasets and training compute budgets. Qwen3-1.7B fine-tuned with <span style="font-family: monospace;">e3</span> extrapolates test-time compute outperforming all &lt; 2B models on AIME'25 and even some larger 7B/32B models.
      </p>    
    </div>
  </div>
</section>
    <!-- End teaser video -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Test-time scaling offers a promising path to improve LLM reasoning by utilizing more compute at inference time; however, the true promise of this paradigm lies in <i>extrapolation</i> (<i>i.e.</i>, improvement in performance on hard problems as LLMs keep "thinking" for longer, beyond the maximum token budget they were trained on). Surprisingly, we find that most existing reasoning models do not extrapolate well.
                We show that one way to enable extrapolation is by training the LLM to perform <i>in-context exploration</i>: training the LLM to effectively spend its test time budget by chaining operations (such as generation, verification, refinement, <i>etc.</i>), or testing multiple hypotheses before it commits to an answer.
                To enable in-context exploration, we identify three key ingredients as part of our recipe <span style="font-family: monospace;">e3</span>: <b>(1)</b> chaining skills that the base LLM has asymmetric competence in, <i>e.g.</i>, chaining verification (easy) with generation (hard), as a way to implement in-context search; <b>(2)</b> leveraging "negative" gradients from incorrect traces to amplify exploration during RL, resulting in longer search traces that chain additional asymmetries; and <b>(3)</b> coupling task difficulty with training token budget during training via a specifically-designed curriculum to structure in-context exploration. Our recipe <span style="font-family: monospace;">e3</span> produces the best known 1.7B model according to AIME/HMMT'25 scores, and extrapolates to 2.5√ó the training token budget. Our <span style="font-family: monospace;">e3</span>-1.7B model does not only attain high pass@1 scores, but also improves pass@k over the base model.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->
    
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <details>
              <summary style="font-size: 2rem; font-weight: bold; cursor: pointer;">Problem Statement: Optimizing & Extrapolating Test-Time Compute</summary>
            <!-- <h2 class="title is-3" id="setup">Problem Statement: Optimizing & Extrapolating Test-Time Compute</h2> -->
              <!-- <h3 class="title is-4">Experiment Setup</h3> -->
              <div class="content has-text-justified">
                <p>
                  In this study, our goal is to train models that can improve performance when we extrapolate test-compute beyond training budget <i>B<sub>tr</sub></i>. 
                  Even though the true promise of test-time compute is extrapolation performance, we find that <b>current thinking models show miniscule gains when extrapolating to 2-3√ó the training budget</b>, as shown in the figure below.
                </p>
                <div style="display: flex; justify-content: center; gap: 20px; align-items: center;"><img
                  src="static/images/aime_line.png"
                  alt="open-source models"
                  width="50%"
                />
                <p>We show that the key to enabling extrapolation is <b>learning to explore in-context</b>: if a model learns to use compute by searching through multiple reasoning paths or implementing procedures, it can "guide" the search towards the correct answer, and improve its performance as more test compute becomes available. To demonstrate this, we build a recipe <span style="font-family: monospace;">e3</span>, which trains models that leverage test compute for in-context exploration and can perform well at both normal training and extrapolation budgets. At its core, <span style="font-family: monospace;">e3</span> is based on three ingredients: <b>(i)</b> base model asymmetries, <b>(ii)</b> negative gradients during RL, and <b>(iii)</b> a coupled data & budget curriculum.</p>
                </p>
              </div>
              </div>
            </div>
          </details>
          </div>
        </div>
      </div>
    </section>
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <details>
              <summary style="font-size: 2rem; font-weight: bold; cursor: pointer;">Final Results with <span style="font-family: monospace;">e3</span>: State-of-the-art &lt; 2B Model on AIME and HMMT‚Äô25</summary>
            <!-- <h2 class="title is-3" id="setup">Final Results with <span style="font-family: monospace;">e3</span>: State-of-the-art &lt; 2B Model on AIME and HMMT‚Äô25 </h2> -->
            <div class="content has-text-justified">
              <p>
                To achieve better extrapolation performance, apply our recipe <span style="font-family: monospace;">e3</span> to the Qwen3-1.7B base model and evaluate it on AIME'25 and HMMT'25 against open-source models.
                As shown, at a test-time token budget of 32k tokens, <span style="font-family: monospace;">e3</span> achieves state-of-the-art performance on AIME'25 and HMMT'25, within a model class of size &lt; 2B. We outperform the best model in this class by &gt; 10% on AIME'25 in terms of peak performance, and show that our model, trained only up to a budget of 16k, extrapolates better than other models including s1.1-32B and OpenThinker-7B when we extrapolate them to 32k output tokens. Moreover, (c) shows that compared to budget forcing via "Wait", <span style="font-family: monospace;">e3</span> achieves substantially better scaling, without any form of prompting or budget forcing.
                <div style="display: flex; gap: 10px; text-align: center; margin-top: 10px; justify-content: center;">
                  <div><img src="static/images/method_plot_aime.png"><div>(a)</div></div>
                  <div><img src="static/images/method_plot_hmmt.png"><div>(b)</div></div>
                  <div><img src="static/images/wait_plot_extrapolation.png"><div>(c)</div></div>
                </div>
              </p>

              <p>
                In the table below, we also report the <i>pass@k</i> performance, comparing <span style="font-family: monospace;">e3</span> with other models of a similar size. We find that our final model at the end of second stage of training on a budget of 16k outperforms other models on higher values of <i>k</i>, on AIME and HMMT '25. We  especially note the comparison against the Nemotron-Reasoning-1.5B model trained with a prolonged RL training recipe on a broader dataset, including our training data.
              </p>
              <table border="1" cellspacing="0" cellpadding="5" style="border-collapse: collapse; text-align: center;">
                <thead>
                  <tr>
                    <th rowspan="2">Model</th>
                    <th colspan="6">AIME 2025</th>
                    <th colspan="6">HMMT 2025</th>
                  </tr>
                  <tr>
                    <th>k=1</th><th>2</th><th>4</th><th>8</th><th>16</th><th>32</th>
                    <th>k=1</th><th>2</th><th>4</th><th>8</th><th>16</th><th>32</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Qwen3-1.7B</td>
                    <td>35.5</td><td>41.4</td><td>47.0</td><td>52.4</td><td>58.3</td><td>65.2</td>
                    <td>22.2</td><td>27.3</td><td>33.0</td><td>39.5</td><td>46.7</td><td>54.9</td>
                  </tr>
                  <tr>
                    <td>R1-distill-Qwen-1.5B</td>
                    <td>23.1</td><td>29.2</td><td>34.5</td><td>40.1</td><td>46.3</td><td>52.5</td>
                    <td>12.5</td><td>19.1</td><td>24.3</td><td>27.9</td><td>36.1</td><td>42.8</td>
                  </tr>
                  <tr>
                    <td>Nemotron-Reasoning-1.5B</td>
                    <td>33.6</td><td>38.5</td><td>43.6</td><td>48.9</td><td>53.8</td><td>58.0</td>
                    <td>17.4</td><td>22.5</td><td>29.6</td><td>35.2</td><td>40.7</td><td>45.0</td>
                  </tr>
                  <tr style="font-weight: bold;">
                    <td><span style="font-family: monospace;">e3</span> (Ours)</td>
                    <td>43.8</td><td>51.1</td><td>56.7</td><td>60.8</td><td>64.0</td><td>67.2</td>
                    <td>24.7</td><td>30.4</td><td>37.0</td><td>44.1</td><td>50.8</td><td>56.1</td>
                  </tr>
                </tbody>
              </table>
            </div>
            </details>
          </div>
        </div>
      </div>
    </section>
    <section class="section" , id="">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <!-- <h2 class="title is-3" id="setup">Ingredient 1: Asymmetries in the Base Model</h2> -->
            <details>
              <summary style="font-size: 2rem; font-weight: bold; cursor: pointer;">Ingredient 1: Asymmetries in the Base Model</summary>
              <div class="content has-text-justified">
                <p>
                  When the base model exhibits asymmetric incompetence at different skills (<i>e.g., </i> when the model is more accurate at verifying its own answers than it is at generating correct ones), RL post-training prefers to learn solutions that <b>chains asymmetries</b> in ways that improve final performance.
                </p>
                <!-- <div class="content has-text-justified tw-bg-blue-100 p-3 rounded-lg text-center mb-4">
                  <p><b>Chaining asymmetric capabilities <i>p</i>, <i>q</i> in model œÄ</b></p>
                  <p>Let <i>p</i>, <i>q</i>: ùíÆ &rarr; ùíÆ be functions over token sequences <span style="font-family: serif;">ùíÆ</span> (<i>e.g.</i>, <i>p</i> can be generation, <i>q</i> can be verification), and <span style="font-family: monospace;">detect(f, œÑ)</span> detects the number of calls to function <i>f</i> in a token trace <i>œÑ</i>. 
                    For a reward <i>r</i>, we say that policy <i>œÄ</i> chains asymmetries <i>p</i>, <i>q</i> if it benefits from calls to the composition <i>q(p(&middot;))</i>, compared to only <i>p(&middot;)</i>:</p>
                    <div class="columns is-centered" style="margin-top: 10px;"><img src="https://latex.codecogs.com/svg.latex?\mathbb{E}_{\tau\sim\pi}[r(\tau)\mid\texttt{detect}(q(p(\cdot)),\tau)>0]\;>\;\mathbb{E}_{\tau \sim \pi}[r(\tau)\mid\texttt{detect}(p,\tau)>0]" alt="" /></div>
                  <p>Even though there is an optimal policy that never calls <i>q</i>.
                </div> -->

                <div style="display: block; justify-content: center; gap: 20px; align-items: center;"><img
                  src="static/images/combined_vg_gap_with_verification.png"
                  alt=""
                  width="100%"
                />
                <p style="margin-top: 10px;">
                  As shown above, when asymmetries such as the VG gap are present (<i>e.g.,</i> in base models for the Countdown task), RL training amplifies response length by chaining more asymmetries to explore in-context, where the probability of success improves with higher length on both training and extrapolation regimes. On the other hand, when VG gap is absent in the base model (<i>e.g.,</i> in the multiplication task), increases in length and extrapolation performance are subdued. When we explicitly train on a base model fine-tuned to verify multiplication (Mult w. verify), we again observe upward length and extrapolation trends also seen in Countdown.
                </p>
            </div>
            <div class="content has-text-justified tw-bg-orange-100 p-3 rounded-lg text-center mb-4" style="margin-top: 10px;">
            <b>Key Findings</b>
              <ul>
                <li>Asymmetries like the VG gap enable the model to continually explore, verify, and refine answers.</li>
                <li>RL training amplifies chaining of asymmetric skills and produces solutions that learn to explore
                  in-context, thus benefiting from additional test-time compute beyond the training budget.</li>
              </ul>
            </div>
          </details>
          </p>
          </div>
          </div>
        </div>
      </div>
    </section>
    <section class="section" , id="">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <details>
              <summary style="font-size: 2rem; font-weight: bold; cursor: pointer;">Ingredient 2: Negative Gradient in RL Enables Chaining</summary>
            <div class="content has-text-justified">
                <p>
                  Most RL post-training methods such as REINFORCE, PPO, and GRPO adopt the following policy gradient update rule:
                  <div class="columns is-centered" style="margin-top: 10px;"><img src="https://latex.codecogs.com/svg.image?\mathbb{E}_{\mathbf{y}\sim\tilde{\pi}(\cdot\mid\mathbf{x})}\;[A_i(\mathbf{x},\mathbf{y})\cdot\nabla_\pi\log\pi(\mathbf{y}\mid\mathbf{x})]" alt="" /></div>
                  
                  We refer to gradient terms multiplied by a negative advantage in the equation above as <b>negative gradients</b>. To understand how negative gradients can help with in-context exploration, consider the effect of negative gradients on an incorrect response <b><i>y</i></b> with tokens <i>y<sub>1</sub></i>, <i>y<sub>2</sub></i>, ..., <span style="font-family: monospace;">EOS</span>. Negative gradients <i>push down</i> the conditional probability <i>p(y<sub>i</sub>|<b>y<sub>1:i-1</sub></b>)</i>, which promotes response diversity (and thereby exploration) across gradient steps because it encourages the model to output different tokens than it had previously. This process also reduces the probability <i>p(<span style="font-family: monospace;">EOS</span> |<b>y</b>)</i> for any incorrect response that ends within the response budget, and moves probability lost here to other places, <i>e.g.,</i> continuing with ‚ÄúWait, ...‚Äù instead of terminating with <span style="font-family: monospace;">EOS</span>.
                </p>
                <p>
                  To verify this empirically, we compare RL post-training with and without negative gradients. In the latter setting, we zero out negative gradients during training whilst retaining the positive gradient.
                </p>
                
                <div style="display: block; justify-content: center; gap: 20px; align-items: center;"><img
                  src="static/images/ng_all.png"
                  alt=""
                  width="100%"
                  />
                  <p style="margin-top: 10px;">
                    As shown above, negative gradients amplify the chaining of asymmetries in two settings where the VG gap is large, leading to longer response lengths and improved extrapolation performance. 
                    This is because amplifying asymmetries incentivizes the model to explore more, which in turn allows it to discover and chain additional asymmetries that improve its final answer.
                    Negative gradients also promote diverse responses, as shown by the higher per-token entropy.
                  </p>
                </div>
                <div class="content has-text-justified tw-bg-orange-100 p-3 rounded-lg text-center mb-4" style="margin-top: 10px;">
                  <b>Key Findings</b>
                  <ul>
                    <li>Negative gradients in RL ‚Äúmove‚Äù probability from short-length incorrect answers onto other
                      modes, e.g., those that exploit asymmetries or those that end in a correct answer. When the VG
                      gap is large, longer responses that chain more asymmetries and eventually discover the right
                      answer are rewarded and reinforced, encouraging in-context exploration. </li>
                    <li>Negative gradients boost response diversity and thus coverage over correct answers.</li>
                  </ul>
                </div>
                </details>
                </div>
              </div>
            </div>
            </section>
            
            
            <section class="section" , id="">
              <div class="container is-max-desktop">
                <div class="columns is-centered">
                  <div class="column is-full-width">
                    <details>
                    <summary style="font-size: 2rem; font-weight: bold; cursor: pointer;">Ingredient 3: Coupled Curricula Training Structures Exploration in RL</summary>
                    <!-- <h2 class="title is-3" id="">Ingredient 3: Coupled Curricula Training Structures Exploration in RL</h2> -->
                    <h3 class="title is-4">Training on Static Budgets or Data Mixtures is Insufficient</h3>
                      <div class="content has-text-justified">
                        <p>
                          In the presence of asymmetries, training with negative gradients produces models that can extrapolate
                          beyond their training budget. However, we show that training on just any arbitrarily chosen training token
                          budget <i>B<sub>tr</sub></i> with asymmetries and negative gradients is not enough.
                          <div style="display: flex; gap: 10px; text-align: center; margin-top: 10px; justify-content: center; align-items: center; align-self: center;">
                            <div style="width: 35%;"><img src="static/images/static_data_vary_budget.png"><div>(a)</div></div>
                            <div style="width: 35%;"><img src="static/images/static_budget_vary_data.png"><div>(b)</div></div>
                          </div>
                        </p>
                        <p>
                          <b>Training solely at low or high <i>B<sub>tr</sub></i> is not desirable.</b> As shown in (a), the training budget that leads to best extrapolation accuracy is 8k. Training at the short budget <i>B<sub>tr</sub></i> = 4k ‚Äúkills‚Äù in-context exploration since traces with more asymmetries that are longer than the training budget of 4k are negatively rewarded. 
                          On the other extreme, training at <i>B<sub>tr</sub></i> = 16k introduces significant optimization challenges, typical of policy gradients in long horizons suffering from high gradient variance.
                        </p>
                        <p>
                          <b>Training on a static data mixture is insufficient.</b> We compare the naive training data mixture with equal proportions of all difficulties (easy + medium + hard) against easy, easy + medium  at <i>B<sub>tr</sub></i> = 8k. As shown in (b), the model trained on only easy problems obtains the best performance on OOD AIME‚Äô25 when extrapolating compute to 32k. This is because responses to harder problems require larger budgets, so training on harder problems leads to training on budgets smaller than the length of a typical response, which penalizes in-context exploration early in training.
                        </p>
                      </div>
                    <h3 class="title is-4">Our Recipe <span style="font-family: monospace;">e3</span>: Coupled Curriculum for In-Context Exploration</h3>
                      <div class="content has-text-justified">
                        <p>
                          So how should we choose the training budget <i>B<sub>tr</sub></i>? We simplify curriculum design by first fixing the dataset at each stage and progressively increasing task difficulty in a stage-wise manner, from easy to hard. Now, at each curriculum stage <i>i</i>, we define a dataset <i>D<sub>i</sub></i> and focus on selecting an appropriate token budget <i>B<sub>tr,i</sub></i>. The goal is to choose <i>B<sub>tr,i</sub></i> such that training with this budget encourages <b>in-context exploration</b>. To do so, we need to strike a balance between long <i>B<sub>tr,i</sub></i> that causes more difficult optimization / credit assignment and short <i>B<sub>tr,i</sub></i> that leads to less in-context exploration: our heuristic is to choose the <i>smallest</i> budget <i>B<sub>tr,i</sub></i> such that the model is positively rewarded for chaining more asymmetries at a budget of 2 ¬∑ <i>B<sub>tr,i</sub></i> compared to <i>B<sub>tr,i</sub></i>. 
                        </p>
                          <!-- <div class="columns is-centered" style="margin-top: 10px;"><img src="https://latex.codecogs.com/svg.image?\text{argmin}_{B\geq&space;B_0}B\;\;\text{s.t.}\;\;J(\pi_i;D_i,2\cdot&space;B)\;\leq\;\kappa\cdot&space;J(\pi_i;D_i,B),\;\;\kappa>1" alt="" /></div>

                          where <i>J(œÄ ; D , B)</i> denotes the performance of œÄ at budget <i>B</i> on dataset <i>D</i>,  and the budget <i>B<sub>0</sub></i> denotes a reasonable minimal length for œÄ on dataset <i>D<sub>i</sub></i>. Œ∫ represents the minimum ratio of performance at budget <i>2B</i> to that at <i>B</i> such that we consider it a significant enough improvement.
                        </p> -->
                        <!-- <p>
                          <div style="display: flex; justify-content: center; gap: 20px; align-items: center;"><img
                            src="static/images/countdown_curriculum_formula.png"
                            alt=""
                            width="3cd 0%"
                          />
                          To show the effectiveness of our curriculum design, we apply this curriculum to the Countdown task, where we initially train on easy problems (with 3, 4 numbers) at a budget of 256. Following the optimization objective above, to select the optimal budget for the next stage, we examine the performance of this first-stage model on the second-stage dataset consisting of harder problems with 7 numbers. We show that picking according to the optimization objective above leads to a budget of 1024, which indeed leads to the best post-trained extrapolation performance on the harder problems.
                        </p> -->
                      <p>
                        Following this insight, we choose the first stage budget to be 8k so that it has both sufficient exploration compared to 4k and is not as difficult to optimize as 16k, as observed in (a) of the graph in the previous subsection.
                      </p>
                      </div>
              
            <div class="content has-text-justified tw-bg-orange-100 p-3 rounded-lg text-center mb-4" style="margin-top: 10px;">

            
              <b>Key Findings</b>
              <ul>
                <li>RL with fixed training budget / data mixture hurts in-context exploration</li>
                <li>We propose a coupled curriculum <span style="font-family: monospace;">e3</span>: at each stage, given data mixture <i>D</i>, choose smallest <i>B<sub>tr</sub></i> such that chaining more asymmetries till a budget of 2 ¬∑ <i>B<sub>tr</sub></i> is positively rewarded at RL initialization.</li>
                <li>By fine-tuning Qwen3-1.7B with <span style="font-family: monospace;">e3</span> we outperform &lt; 2B models on AIME‚Äô25, HMMT ‚Äô25.</li>
              </ul>
            </div>
          </details>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <details>
              <summary style="font-size: 2rem; font-weight: bold; cursor: pointer;">Example Traces and Asymmetries</summary>
            <!-- <h2 class="title is-3" id="setup">Final Results with <span style="font-family: monospace;">e3</span>: State-of-the-art &lt; 2B Model on AIME and HMMT‚Äô25 </h2> -->
            <div class="content has-text-justified">
                <p>
                <details style="max-width: 100%; overflow-x: auto;">
                  <summary style="font-weight: bold; cursor: pointer;">Example of chained asymmetries in Countdown</summary>
                    <pre><code>&lt;think&gt; Let me analyze this step by step:
First, let‚Äôs try to get close to 466 with simple operations: 
65 + 56 = 121 (too small)
65 - 56 = 9 (too small)
65 * 56 = way too large
56 * 65 = way too large

Let‚Äôs try working backwards from 466: 466 is much larger than any of the given
numbers, so multiplication will likely be needed. We need to get from 37 to 466, which
means we need to multiply by 12.5. But we don‚Äôt have 12.5, so we‚Äôll need to create it
through division.

Let‚Äôs try: 
37 * 14 = 518 (too large) 
37 - 14 = 23 (too small) 
37 + 14 = 51 (too small)

Let‚Äôs try a different approach: 
65 + 61 = 126
126 + 56 = 182
182 + 37 = 219 (getting closer)

Let‚Äôs try: 
65 + 61 = 126
126 + 56 = 182
182 + 37 = 219
219 + 14 = 233 (still too small)

Let‚Äôs try one more approach: 
65 + 56 = 121 
121 + 61 = 182 
182 + 37 = 219 
219 + 14 = 233 (still not 466)

Let‚Äôs try: 
65 + 56 = 121 
121 + 61 = 182 
182 + 37 = 219 
219 + 14 = 233 (still not 466)

One more attempt: 
65 + 61 = 126 
126 + 37 = 163 
163 + 56 = 219 
219 + 14 = 233 (still not 466)

Final try: 
65 + 61 = 126 
126 + 56 = 182 
182 + 37 = 219 
219 - 14 = 205 (not quite there)

Let‚Äôs try: 
65 + 61 = 126 
126 + 56 = 182 
182 + 37 = 219 
219 + 14 = 233 (still not 466)

New approach: 
65 + 61 = 126 
126 + 56 = 182 
182 - 37 = 145 
145 + 14 = 159 (not quite there)

Final attempt: 
65 + 61 = 126 
126 + 56 = 182 
182 + 37 = 219 
219 - 14 = 205 (not quite there)


Let‚Äôs try: 
65 + 56 = 121 
121 + 61 = 182 
182 + 37 = 219 
219 - 14 = 205 (not quite there)

One last try: 
65 + 37 = 102 
102 + 61 = 163 
163 + 56 = 219 
219 + 14 = 233 (not quite there)

Final attempt: 
56 + 14 = 70 
70 + 65 = 135 
135 + 37 = 172 
172 + 61 = 233 (not quite there)

New approach: 
37 * 14 = 
518 518 - 65 = 453 
453 + 61 = 514 
514 - 56 = 458 (not quite there)

Final try: 
61 * 14 = 
854 854 - 65 = 789 
789 - 56 = 733 
733 - 37 = 696 (not quite there)

Let‚Äôs try: 
37 * 14 = 
518 518 - 65 = 453 
453 + 61 = 514 
514 - 56 = 458 (not quite there)

New approach: 
56 * 37 = 2,052 
2,052 / 65 approx. 31.5 (not helpful)

Final attempt: 
65 * 14 = 
910 910 - 61 = 849 
849 - 56 = 793 
793 + 37 = 830 (not quite there)

New approach: 
61 * 37 = 2,237 
2,237 - 65 = 1,172 
1,172 - 56 = 1,116 
1,116 + 14 = 1,130 (not quite there)

New approach: 
37 * 14 = 518 
518 + 65 = 583 
583 - 61 = 522 
522 + 56 = 578 (not quite there)

Final attempt: 
65 * 14 = 910 
910 - 61 = 849 
849 + 56 = 905 
905 - 37 = 868 (not quite there)

New approach: 
37 * 14 = 518 
518 - 56 = 462 
462 + 65 = 527 
527 - 61 = 466 (This works!)
&lt;/think&gt;
&lt;answer&gt; (37 * 14 - 56 + 65) - 61 &lt;/answer&gt;</code></pre></details></p>
               <p>
                <details style="max-width: 100%; overflow-x: auto;">
                  <summary style="font-weight: bold; cursor: pointer;">Example of chained asymmetries in Multiply w. verify</summary>
                    <pre><code>&lt;think&gt; Okay, so I need to multiply 347 by 583. Hmm, let me think about how to approach this.
I guess I can use the standard multiplication algorithm where you break it down by each
digit. Let me start by writing both numbers down:
347 √ó 583
‚Äì‚Äì‚Äì‚Äì
First, I‚Äôll multiply 347 by each digit of 583 starting from the rightmost digit, which
is 3 in this case. Then move to the next digit (8) and finally the leftmost digit (5),
remembering to add the appropriate zeros for each subsequent multiplication. After
that, I‚Äôll sum all the results together. Let me start with the rightmost digit.
Starting with the units place: 3 √ó 347. Let me compute that:
3 √ó 7 = 21. I write down 1 and carry over 2.
3 √ó 4 = 12, plus the carried over 2 makes 14. Write down 4 and carry over 1.
3 √ó 3 = 9, plus the carried over 1 makes 10. Write down 10.
So, 3 √ó 347 = 1041. Let me check that again:
3 √ó 7 = 21 (correct), 3 √ó 4 = 12 + 2 = 14 (correct), 3 √ó 3 = 9 + 1 = 10 (correct). So
that‚Äôs 1041. Alright.
Check: 3 √ó 347 = 1041. Correct.
Next, the tens place. The digit here is 8, but since it‚Äôs in the tens place, it‚Äôs
actually 80. So, we‚Äôll multiply 347 by 80. To make it easier, I can multiply 347 by 8
and then add a zero at the end.
8 √ó 347:
8 √ó 7 = 56. Write down 6, carry over 5.
8 √ó 4 = 32, plus the carried over 5 makes 37. Write down 7, carry over 3.
8 √ó 3 = 24, plus the carried over 3 makes 27. Write down 27.
So, 8 √ó 347 = 2776. Then add the zero for the tens place, making it 27760. Let me
verify:
8 √ó 347. 8√ó300=2400, 8√ó40=320, 8√ó7=56. 2400+320=2720, 2720+56=2776. Then adding the
zero gives 27760. Correct.
Check: 8 √ó 347 = 2776, so 2776 √ó 10 = 27760. Correct.
Now, the hundreds place. The digit here is 5, but it‚Äôs in the hundreds place, so it‚Äôs
500. So, multiply 347 by 500. Again, multiply 347 by 5 and add two zeros.
5 √ó 347:
5 √ó 7 = 35. Write down 5, carry over 3.
5 √ó 4 = 20, plus the carried over 3 makes 23. Write down 3, carry over 2.
5 √ó 3 = 15, plus the carried over 2 makes 17. Write down 17.
So, 5 √ó 347 = 1735. Then add two zeros, making it 173500. Let me verify:
5 √ó 347. 5√ó300=1500, 5√ó40=200, 5√ó7=35. 1500+200=1700, 1700+35=1735. Then adding two
zeros gives 173500. Correct.
Check: 5 √ó 347 = 1735, so 1735 √ó 100 = 173500. Correct.
Now, add all three results together: 1041 + 27760 + 173500.
Let me do this step by step. First, add 1041 and 27760:
1041 + 27760. Let me line them up:
1041
+27760
‚Äì‚Äì‚Äì‚Äì
28801
Wait, 1041 + 27760: 1000 + 27760 = 28760, then +41 = 28801. Let me check with another
method:
20000 + 7000 = 27000, 700 + 60 = 760, 27000 + 760 = 27760. Then 27760 + 1000 = 28760,
+41 = 28801. Correct.
Check: 1041 + 27760 = 28801. Correct.
Now add this result to 173500:
28801 + 173500. Let me line them up:
28801
+173500
‚Äì‚Äì‚Äì‚Äì-
202301
Wait, 28000 + 173500 = 201500, then 801 + 0 = 801, so total 201500 + 801 = 202301. Let
me verify another way:
28801 + 173500. Break down 173500 into 170000 + 3500. Then 28801 + 170000 = 198801.
Then 198801 + 3500 = 202301. Correct.
Check: 28801 + 173500 = 202301. Correct.
So, putting all together, the final result is 202,301. Let me just cross-verify using
another method to ensure I didn‚Äôt make any mistakes in the addition.
&lt;/think&gt;
The product of 347 and 583 is 202301.</code></pre></details>
                </p>
      
<p>
                <details style="max-width: 100%; overflow-x: auto;">
                  <summary style="font-weight: bold; cursor: pointer;">Example of chained asymmetries in Math</summary>
                    <pre><code>&lt;think&gt; </code></pre></details>
                </p>      
      
              </div>
            </details>
          </div>
        </div>
      </div>

    </section>
    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
@misc{,
      title={e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs},
      author={Amrith Setlur*, Matthew Y. R. Yang*, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, Aviral Kumar},
      year={2025},
      eprint={},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={},
}
        </code></pre>
      </div>
    </section>
    <!--End BibTex citation -->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                Corresponding Author:
                <a href="mailto:asetlur@andrew.cmu.edu">Amrith Setlur</a>
                <br />
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the¬†<a
                  href="https://nerfies.github.io"
                  target="_blank"
                  >Nerfies</a
                >¬†project page.
                <br />
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
</html>
