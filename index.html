<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs" />
    <meta property="og:title" content="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs" />
    <meta
      property="og:description"
      content="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs"
    />
    <meta property="og:url" content="https://matthewyryang.github.io/e3" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/overview.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <meta name="twitter:title" content="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs" />
    <meta
      name="twitter:description"
      content="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs"
    />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta
      name="twitter:image"
      content="static/image/overview.png"
    />
    <meta name="twitter:card" content="static/image/overview.png" />
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>e3</title>
    <link rel="icon" type="image/x-icon" href="static/images/icon.png" />
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="static/css/bulma.min.css" />
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="static/css/index.css" />
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
      tailwind.config = {
        prefix: 'tw-', // This makes all Tailwind classes start with 'tw-'
      };
    </script>
  </head>
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h2 class="title is-2 publication-title ">
                <span style="font-family: monospace;">e3</span>: Learning to <u>E</u>xplore <u>E</u>nables <u>E</u>xtrapolation of Test-Time Compute for LLMs
              </h2>
              <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://ars22.github.io/" target="_blank"
                    >Amrith Setlur<sup>*1</sup></a
                  >
                  ,
                </span>
                <span class="author-block">
                  <a href="https://matthewyryang.com/" target="_blank"
                    >Matthew Y. R. Yang<sup>*1</sup></a
                  >
                  ,
                </span>
                <span class="author-block">
                  <a href="https://sea-snell.github.io/" target="_blank"
                    >Charlie Snell<sup>2</sup></a
                  >
                  ,
                </span>
                <span class="author-block">
                  <a href="https://github.com/jgreer013/" target="_blank"
                    >Jeremy Greer<sup>3</sup></a
                  >
                  ,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=QdfBJmAAAAAJ&hl=en" target="_blank"
                    >Ian Wu<sup>1</sup></a
                  >
                  ,
                </span>
                <span class="author-block">
                  <a href="https://www.cs.cmu.edu/~smithv/" target="_blank"
                    >Virginia Smith<sup>1</sup></a
                  >
                  ,
                </span>
                <span class="author-block">
                  <a href="https://msimchowitz.github.io/" target="_blank"
                    >Max Simchowitz<sup>1</sup></a
                  >
                  ,
                </span>
                <span class="author-block">
                  <a href="https://aviralkumar2907.github.io/" target="_blank"
                    >Aviral Kumar<sup>1</sup></a
                  >
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <sup>1</sup>Carnegie Mellon University,
                  <sup>2</sup>UC Berkeley,
                  <sup>3</sup>Oumi,
                  <sup>*</sup>Equal Contribution
                </span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a
                      href=""
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                  <!-- <span class="link-block">
                    <a
                      href="https://www.youtube.com/watch?v=Qv8aTLthfhs"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a
                      href=""
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="has-text-centered">
        <div class="text-center">
          <img src="static/images/fig1-gif.gif" alt="e3: Exploration Enables Extrapolation" class="mx-auto"/>
        </div>
      </div>
      <p class="tw-text-left tw-text-sm tw-text-gray-500 tw-mt-4">
        <b>In-context <u>e</u>xploration <u>e</u>nables <u>e</u>xtrapolation
        of test-time compute (<span style="font-family: monospace;">e3</span>)</b>:  (i) by chaining asymmetric capabilities of the base model, <i>e.g.,</i> reliably self-verifying responses after generating them; (ii) lengthening model responses by chaining more asymmetries until the correct answer is discovered by utilizing the "negative" part of the RL policy gradient generated from incorrect responses; and (iii) coupling data & budget curricula for RL training that carefully structures exploration by sequentially training models on different datasets and training compute budgets. Qwen3-1.7B fine-tuned with <span style="font-family: monospace;">e3</span> extrapolates test-time compute outperforming all &lt; 2B models on AIME'25 and even some larger 7B/32B models.
      </p>    
    </div>
  </div>
</section>
    <!-- End teaser video -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Test-time scaling offers a promising path to improve LLM reasoning by utilizing more compute at inference time; however, the true promise of this paradigm lies in <i>extrapolation</i> (<i>i.e.</i>, improvement in performance on hard problems as LLMs keep "thinking" for longer, beyond the maximum token budget they were trained on). Surprisingly, we find that most existing reasoning models do not extrapolate well.
                We show that one way to enable extrapolation is by training the LLM to perform <i>in-context exploration</i>: training the LLM to effectively spend its test time budget by chaining operations (such as generation, verification, refinement, <i>etc.</i>), or testing multiple hypotheses before it commits to an answer.
                To enable in-context exploration, we identify three key ingredients as part of our recipe <span style="font-family: monospace;">e3</span>: <b>(1)</b> chaining skills that the base LLM has asymmetric competence in, <i>e.g.</i>, chaining verification (easy) with generation (hard), as a way to implement in-context search; <b>(2)</b> leveraging "negative" gradients from incorrect traces to amplify exploration during RL, resulting in longer search traces that chain additional asymmetries; and <b>(3)</b> coupling task difficulty with training token budget during training via a specifically-designed curriculum to structure in-context exploration. Our recipe <span style="font-family: monospace;">e3</span> produces the best known 1.7B model according to AIME/HMMT'25 scores, and extrapolates to 2.5√ó the training token budget. Our <span style="font-family: monospace;">e3</span>-1.7B model does not only attain high pass@1 scores, but also improves pass@k over the base model.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3" id="setup">Problem Statement: Optimizing & Extrapolating Test-Time Compute</h2>
              <!-- <h3 class="title is-4">Experiment Setup</h3> -->
              <div class="content has-text-justified">
                <p>
                  In this study, our goal is to train models that can improve performance when we extrapolate test-compute beyond training budget <i>B<sub>tr</sub></i>. 
                  Even though the true promise of test-time compute is extrapolation performance, we find that <b>current thinking models show miniscule gains when extrapolating to 2-3√ó the training budget</b>, as shown in the figure below.
                </p>
                <div style="display: flex; justify-content: center; gap: 20px; align-items: center;"><img
                  src="static/images/aime_line.png"
                  alt="open-source models"
                  width="50%"
                />
                <p>We show that the key to enabling extrapolation is <b>learning to explore in-context</b>: if a model learns to use compute by searching through multiple reasoning paths or implementing procedures, it can "guide" the search towards the correct answer, and improve its performance as more test compute becomes available. To demonstrate this, we build a recipe <span style="font-family: monospace;">e3</span>, which trains models that leverage test compute for in-context exploration and can perform well at both normal training and extrapolation budgets. At its core, <span style="font-family: monospace;">e3</span> is based on three ingredients: <b>(i)</b> base model asymmetries, <b>(ii)</b> negative gradients during RL, and <b>(iii)</b> a coupled data & budget curriculum.</p>
                </p>
              </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="section" , id="">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3" id="setup">Asymmetries in the Base Model: A Prerequisite for In-Context Exploration</h2>
              <div class="content has-text-justified">
                <p><b>Setup.</b> We perform experiments on two didactic settings on top of Math reasoning: <b> (1) Countdown game</b>, where the model must generate a sequence of operations to reach a target number from a given set of numbers; and
                <b>(2) N-digit multiplication</b>, where the model must multiply two n-digit numbers.
                </p>
                <p>
                  When the base model exhibits asymmetric incompetence at different skills (<i>e.g., </i> when the model is more accurate at verifying its own answers than it is at generating correct ones), RL post-training prefers to learn solutions that <b>chains asymmetries</b> in ways that improve final performance.
                </p>
                <!-- <div class="content has-text-justified tw-bg-blue-100 p-3 rounded-lg text-center mb-4">
                  <p><b>Chaining asymmetric capabilities <i>p</i>, <i>q</i> in model œÄ</b></p>
                  <p>Let <i>p</i>, <i>q</i>: ùíÆ &rarr; ùíÆ be functions over token sequences <span style="font-family: serif;">ùíÆ</span> (<i>e.g.</i>, <i>p</i> can be generation, <i>q</i> can be verification), and <span style="font-family: monospace;">detect(f, œÑ)</span> detects the number of calls to function <i>f</i> in a token trace <i>œÑ</i>. 
                    For a reward <i>r</i>, we say that policy <i>œÄ</i> chains asymmetries <i>p</i>, <i>q</i> if it benefits from calls to the composition <i>q(p(&middot;))</i>, compared to only <i>p(&middot;)</i>:</p>
                    <div class="columns is-centered" style="margin-top: 10px;"><img src="https://latex.codecogs.com/svg.latex?\mathbb{E}_{\tau\sim\pi}[r(\tau)\mid\texttt{detect}(q(p(\cdot)),\tau)>0]\;>\;\mathbb{E}_{\tau \sim \pi}[r(\tau)\mid\texttt{detect}(p,\tau)>0]" alt="" /></div>
                  <p>Even though there is an optimal policy that never calls <i>q</i>.
                </div> -->

                <div style="display: block; justify-content: center; gap: 20px; align-items: center;"><img
                  src="static/images/combined_vg_gap_with_verification.png"
                  alt=""
                  width="100%"
                />
                <p style="margin-top: 10px;">
                  As shown above, when asymmetries such as the VG gap are present (<i>e.g.,</i> in base models for the Countdown task), RL training amplifies response length by chaining more asymmetries to explore in-context, where the probability of success improves with higher length on both training and extrapolation regimes. On the other hand, when VG gap is absent in the base model (<i>e.g.,</i> in the multiplication task), increases in length and extrapolation performance are subdued. When we explicitly train on a base model fine-tuned to verify multiplication (Mult w. verify), we again observe upward length and extrapolation trends also seen in Countdown.
                </p>
            </div>
            <div class="content has-text-justified tw-bg-orange-100 p-3 rounded-lg text-center mb-4" style="margin-top: 10px;">
            <b>Key Findings</b>
              <ul>
                <li>Asymmetries like the VG gap enable the model to continually explore, verify, and refine answers.</li>
                <li>RL training amplifies chaining of asymmetric skills and produces solutions that learn to explore
                  in-context, thus benefiting from additional test-time compute beyond the training budget.</li>
              </ul>
            </div>
        </div>
      </div>
    </section>
    <section class="section" , id="">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3" id="setup">Negative Gradients Incentivize Exploration that Chains Asymmetries</h2>
            <div class="content has-text-justified">
                <p>
                  Most RL post-training methods such as REINFORCE, PPO, and GRPO adopt the following policy gradient update rule:
                  <div class="columns is-centered" style="margin-top: 10px;"><img src="https://latex.codecogs.com/svg.image?\mathbb{E}_{\mathbf{y}\sim\tilde{\pi}(\cdot\mid\mathbf{x})}\;[A_i(\mathbf{x},\mathbf{y})\cdot\nabla_\pi\log\pi(\mathbf{y}\mid\mathbf{x})]" alt="" /></div>
                  
                  We refer to gradient terms multiplied by a negative advantage in the equation above as <b>negative gradients</b>. To understand how negative gradients can help with in-context exploration, we compare RL post-training with and without negative gradients. We remove negative gradients zeroing them out during training whilst retaining the positive gradient.
                </p>
                
                <div style="display: block; justify-content: center; gap: 20px; align-items: center;"><img
                  src="static/images/ng_all.png"
                  alt=""
                  width="100%"
                  />
                  <p style="margin-top: 10px;">
                    As shown above, negative gradients amplify the chaining of asymmetries in two settings where the VG gap is large, leading to longer response lengths and improved extrapolation performance. 
                    This is because amplifying asymmetries incentivizes the model to explore more, which in turn allows it to discover and chain additional asymmetries that improve its final answer.
                    Negative gradients also promote diverse responses, as shown by the higher per-token entropy.
                  </p>
                </div>
                <div class="content has-text-justified tw-bg-orange-100 p-3 rounded-lg text-center mb-4" style="margin-top: 10px;">
                  <b>Key Findings</b>
                  <ul>
                    <li>Negative gradients in RL ‚Äúmove‚Äù probability from short-length incorrect answers onto other
                      modes, e.g., those that exploit asymmetries or those that end in a correct answer. When the VG
                      gap is large, longer responses that chain more asymmetries and eventually discover the right
                      answer are rewarded and reinforced, encouraging in-context exploration. </li>
                      <li>Negative gradients boost response diversity and thus coverage over correct answers.</li>
                    </ul>
                  </div>
                </div>
              </div>
            </section>
            <section class="section" , id="">
              <div class="container is-max-desktop">
                <div class="columns is-centered">
                  <div class="column is-full-width">
                    <h2 class="title is-3" id="">Coupled Curriculum Training Structures Exploration in Long Length RL</h2>
                    <h3 class="title is-4">Training on Static Budgets or Data Mixtures is Insufficient</h3>
                      <div class="content has-text-justified">
                        <p>
                        In the presence of asymmetries, training with negative gradients produces models that can extrapolate
                        beyond their training budget. However, we show that training on just any arbitrarily chosen training token
                        budget <i>B<sub>tr</sub></i> with asymmetries and negative gradients is not enough.
                        <div style="display: flex; gap: 10px; text-align: center; margin-top: 10px;">
                          <div><img src="static/images/static_data_vary_budget.png"><div>(a)</div></div>
                          <div><img src="static/images/static_budget_vary_data.png"><div>(b)</div></div>
                          <div><img src="static/images/lenhist_change_budget.png"><div>(c)</div></div>
                          <div><img src="static/images/lenhist_change_data.png"><div>(d)</div></div>
                        </div>
                        <p>
                          <b>(a), (c): Training solely at low or high <i>B<sub>tr</sub></i> is not desirable.</b> Training at the short budget <i>B<sub>tr</sub></i> = 4k ‚Äúkills‚Äù in-context exploration since traces with more asymmetries that are longer than the training budget of 4k are negatively rewarded. 
                          On the other extreme, training at <i>B<sub>tr</sub></i> = 16k introduces significant optimization challenges, typical of policy gradients in long horizons suffering from high gradient variance.
                        </p>
                        <p>
                          <b>(b), (d): Training on a static data mixture is insufficient.</b> We compare the naive training data mixture with equal proportions of all difficulties (easy + medium + hard) against easy, easy + medium  at <i>B<sub>tr</sub></i> = 8k. As shown in (b), the model trained on only easy problems obtains the best performance on OOD AIME‚Äô25 when extrapolating compute to 32k, because training on budgets smaller than the length of a typical response for the base model on that dataset penalizes in-context exploration early in training. 
                        </p>
                      </div>
                    <h3 class="title is-4">Our Recipe <span style="font-family: monospace;">e3</span>: Coupled Curriculum for In-Context Exploration</h3>
                      <div class="content has-text-justified">
                        <p>
                          So how should we choose the training budget <i>B<sub>tr</sub></i> and correspondingly, the prompt mixture for a given budget? We simplify curriculum design by fixing the dataset at each stage and progressively increasing task difficulty in a stage-wise manner, from easy to hard. Now, at each curriculum stage <i>i</i>, we define a dataset <i>D<sub>i</sub></i> and focus on selecting an appropriate token budget <i>B<sub>tr,i</sub></i>. The goal is to choose <i>B<sub>tr,i</sub></i> such that training with this budget encourages <b>in-context exploration</b>. We formulate this as the following optimization problem:
                          <div class="columns is-centered" style="margin-top: 10px;"><img src="https://latex.codecogs.com/svg.image?\text{argmin}_{B\geq&space;B_0}B\;\;\text{s.t.}\;\;J(\pi_i;D_i,2\cdot&space;B)\;\leq\;\kappa\cdot&space;J(\pi_i;D_i,B),\;\;\kappa>1" alt="" /></div>

                          where <i>J(œÄ ; D , B)</i> denotes the performance of œÄ at budget <i>B</i> on dataset <i>D</i>,  and the budget <i>B<sub>0</sub></i> denotes a reasonable minimal length for œÄ on dataset <i>D<sub>i</sub></i>. Œ∫ represents the minimum ratio of performance at budget <i>2B</i> to that at <i>B</i> such that we consider it a significant enough improvement.
                        </p>
                        <p>
                          <div style="display: flex; justify-content: center; gap: 20px; align-items: center;"><img
                            src="static/images/countdown_curriculum_formula.png"
                            alt=""
                            width="30%"
                          />
                          To show the effectiveness of our curriculum design, we apply this curriculum to the Countdown task, where we initially train on easy problems (with 3, 4 numbers) at a budget of 256. Following the optimization objective above, to select the optimal budget for the next stage, we examine the performance of this first-stage model on the second-stage dataset consisting of harder problems with 7 numbers. We show that picking according to the optimization objective above leads to a budget of 1024, which indeed leads to the best post-trained extrapolation performance on the harder problems.
                        </p>
                      </div>
              

            <h3 class="title is-4">Final Results with <span style="font-family: monospace;">e3</span>: State-of-the-art &lt; 2B Model on AIME and HMMT‚Äô25 </h3>
            <div class="content has-text-justified">
              <p>
                We apply our recipe <span style="font-family: monospace;">e3</span> to the Qwen3-1.7B base model and evaluate it on AIME'25 and HMMT'25 against open-source models.
                As shown, at a test-time token budget of 32k tokens, <span style="font-family: monospace;">e3</span> achieves state-of-the-art performance on AIME'25 and HMMT'25, within a model class of size &lt; 2B. We outperform the best model in this class by &gt; 10% on AIME'25 in terms of peak performance, and show that our model, trained only up to a budget of 16k, extrapolates better than other models including s1.1-32B and OpenThinker-7B when we extrapolate them to 32k output tokens. Moreover, (c) shows that compared to budget forcing via "Wait", <span style="font-family: monospace;">e3</span> achieves substantially better scaling, without any form of prompting or budget forcing.
                <div style="display: flex; gap: 10px; text-align: center; margin-top: 10px; justify-content: center;">
                  <div><img src="static/images/method_plot_aime.png"><div>(a)</div></div>
                  <div><img src="static/images/method_plot_hmmt.png"><div>(b)</div></div>
                  <div><img src="static/images/wait_plot_extrapolation.png"><div>(c)</div></div>
                </div>
              </p>

              <p>
                In the table below, we also report the <i>pass@k</i> performance, comparing <span style="font-family: monospace;">e3</span> with other models of a similar size. We find that our final model at the end of second stage of training on a budget of 16k outperforms other models on higher values of <i>k</i>, on AIME and HMMT '25. We  especially note the comparison against the Nemotron-Reasoning-1.5B model trained with a prolonged RL training recipe on a broader dataset, including our training data.
              </p>
              <table border="1" cellspacing="0" cellpadding="5" style="border-collapse: collapse; text-align: center;">
                <thead>
                  <tr>
                    <th rowspan="2">Model</th>
                    <th colspan="6">AIME 2025</th>
                    <th colspan="6">HMMT 2025</th>
                  </tr>
                  <tr>
                    <th>k=1</th><th>2</th><th>4</th><th>8</th><th>16</th><th>32</th>
                    <th>k=1</th><th>2</th><th>4</th><th>8</th><th>16</th><th>32</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Qwen3-1.7B</td>
                    <td>35.5</td><td>41.4</td><td>47.0</td><td>52.4</td><td>58.3</td><td>65.2</td>
                    <td>22.2</td><td>27.3</td><td>33.0</td><td>39.5</td><td>46.7</td><td>54.9</td>
                  </tr>
                  <tr>
                    <td>R1-distill-Qwen-1.5B</td>
                    <td>23.1</td><td>29.2</td><td>34.5</td><td>40.1</td><td>46.3</td><td>52.5</td>
                    <td>12.5</td><td>19.1</td><td>24.3</td><td>27.9</td><td>36.1</td><td>42.8</td>
                  </tr>
                  <tr>
                    <td>Nemotron-Reasoning-1.5B</td>
                    <td>33.6</td><td>38.5</td><td>43.6</td><td>48.9</td><td>53.8</td><td>58.0</td>
                    <td>17.4</td><td>22.5</td><td>29.6</td><td>35.2</td><td>40.7</td><td>45.0</td>
                  </tr>
                  <tr style="font-weight: bold;">
                    <td><span style="font-family: monospace;">e3</span> (Ours)</td>
                    <td>43.8</td><td>51.1</td><td>56.7</td><td>60.8</td><td>64.0</td><td>67.2</td>
                    <td>24.7</td><td>30.4</td><td>37.0</td><td>44.1</td><td>50.8</td><td>56.1</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="content has-text-justified tw-bg-orange-100 p-3 rounded-lg text-center mb-4" style="margin-top: 10px;">

            
              <b>Key Findings</b>
              <ul>
                <li>RL with fixed training budget / data mixture hurts in-context exploration</li>
                <li>We propose a coupled curriculum <span style="font-family: monospace;">e3</span>: at each stage, given data mixture <i>D</i>, choose smallest <i>B<sub>tr</sub></i> such that chaining more asymmetries till a budget of 2 ¬∑ <i>B<sub>tr</sub></i> is positively rewarded at RL initialization.</li>
                <li>By fine-tuning Qwen3-1.7B with <span style="font-family: monospace;">e3</span> we outperform &lt; 2B models on AIME‚Äô25, HMMT ‚Äô25.</li>
              </ul>
            </div>
        </div>
      </div>
    </section>
    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
@misc{,
      title={e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs},
      author={Amrith Setlur*, Matthew Y. R. Yang*, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, Aviral Kumar},
      year={2025},
      eprint={},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={},
}
        </code></pre>
      </div>
    </section>
    <!--End BibTex citation -->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                Corresponding Author:
                <a href="mailto:asetlur@andrew.cmu.edu">Amrith Setlur</a>
                <br />
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the¬†<a
                  href="https://nerfies.github.io"
                  target="_blank"
                  >Nerfies</a
                >¬†project page.
                <br />
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
</html>
